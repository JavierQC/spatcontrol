### the precision matrix
The significance of the variance of xi|x-i in terms of relaxed dependency when isolated.
The threshold makes our gaussian field to become a gaussian markov random field, we should calculate the discrepancy by the Kullback±Leibler discrepancy measures, for the final values of the parameters (Rue2002).
Or simply we can tell that applying the threshold at 100, given our posterior f and T correspond to neglecting <<1% of the spatial component:
the reasoning is in two steps as having the full matrix distance is impossible, first the very broad maximization of the sum of the weight over 200m : nbhouses*meanT*exp(-200/meanf)<<1 o/oo of the sum of mean sum of weight so it can be neglected. the same with 100 is not true so:

dist_mat100 <-nearest.dist(x=data[,c("easting","northing")], y=NULL, method="euclidian", delta=100, upper=NULL)
dist_mat200 <-nearest.dist(x=data[,c("easting","northing")], y=NULL, method="euclidian", delta=200, upper=NULL)

spam.options(nearestdistnnz=c(13764100,400))
SB <- nearest.dist(x=cbind(data$block_num,rep(0,length(data$block_num))), method="euclidian", upper=NULL,delta=0.1)
SB@entries<-rep(1,length(SB@entries))
dmt<-dist_mat100
dmt@entries<-rep(1,length(dmt@entries))# [dmt@entries!=0]<-1 # 1 only when dist_mat not 0
SB100<-SB
SB100@entries<-rep(1,length(SB100@entries))
SB100<-logic_and_spam(SB100,dmt);

AS100<-dmt-SB100; # get 1 whereever the distances matrix is defined(under threshold) and not same block
AS100<-as.spam(AS100)

dmt<-dist_mat200
dmt@entries<-rep(1,length(dmt@entries))# [dmt@entries!=0]<-1 # 1 only when dist_mat not 0
SB200<-SB
SB200@entries<-rep(1,length(SB200@entries))
SB200<-logic_and_spam(SB200,dmt);

AS200<-dmt-SB200; # get 1 whereever the distances matrix is defined(under threshold) and not same block
AS200<-as.spam(AS100)

weights100<- exp(-dist_mat100*SB100/meanf)+meanT*exp(-dist_mat100*AS100/meanf)
weights200<- exp(-dist_mat200*SB200/meanf)+meanT*exp(-dist_mat200*AS200/meanf)

sw100<-weights100%*%rep(1,nrow(weights100))
sw200<-weights200%*%rep(1,nrow(weights200))

mean((sw200-sw100)/sw200) 

this is 7.022614e-05 for one of the simulation on north west mariano melgar, the max of this value being 1e-3
so no problem to neglect the out of 100, even if we count a much broader map.

### the median normalization
It is known that the posterior using an exponential kernel can easily be improper
 [James O. Berger, Victor de Oliveira, Bruno Sansó. Objective Bayesian Analysis of Spatially Correlated Data. Journal of the American Statistical Association, Vol. 96, No. 456 (Dec., 2001), pp. 1361-1374]
 but I don't think we use the same model.  

Anyway without it we got much too much correlation between Ku and f/T and when Ku is adjusted it's very difficult to move from it. So the transformation we propose allows to avoid this.

Moreover, this allows the likelihood of f/T not to be flat on big values.

# is sampling ok for metropolis hasting
The sampling is done using the lognormal, like it's prior. Should we use something more corresponding to the shape of T (inverse)?

### adding v and model selection
# 
# the model with u is very good, with a likelihood of z|w of -1271.324 that translate into a mean probability (quality of prediction) for each point: exp(mean(sampled[-(1:5000),7])/nrow(data))
of 0.852319

# and the model with v is no better with a mean likelihood of z|w of -1318.976 that translate into a mean probability for each point of:
0.8472294

the difference is much more than what an Akaike criterium would accept but we can think that this it has to be done to put noise in the model

It would arrange our things as it allows the mean effect of streets to move from 0.2431908 to 0.06673058, the way I understand it is that when you add the noise in the model, that makes the spatial parameter to be less dependant on contingent house variations and then to fit better the really spatial part. But on the likelyhood side, this makes the output w less well fitting to the data, by construction. The question for me is if we can use the model selection only on the cofactors and not on v? I guess that yes but it is not that obvious.

Should we test the model without u?

### general issues in model selection
- if we want to do model selection (on T, kernels, v, covactors) what likelihood should we be looking at? 
in simulation it would be the similarity between the data and the final product of the model
in statistics it's a  little more complex, but I guess that we can select using the final product of the part we want to evaluate:
	- if we want to evaluate the interest of v, we should be using everything out of pi(Kv) and pi(v|Kv) 
	- if we want to evaluate the interest of T, we should evaluate everything but pi(T)

The other problem is if we should do this evaluation on the mean values of everything or use the mean of the likelihood of all the realisations,
It may be kind of different

Would we use more a bayes factor? how to integrate on each factor value? I would do a numerical approximate grid estimation, but should we refit the model everytime ? There isn't a way to use the posteriors directly or the likelihood distribution?

### kernels model selection
For model selection, look at Finley2007.
If we can do model selection, it would be good to test:
- gaussian kernel to allow for short range important dispersion. Using sd as the "f parameter", it may improve the short range description of the autocorrelation (but this has to be done with last results).
- mattern kernel as it is a recommended function for covariance matrices. Probably fixing the phi parameter or testing 2/3 values with Kappa the equivalent of f.
For this two models the definition of T doesn't change.

### the autocorrelation reproduction
The autocorrelation is no too badly reproduced, including high difference in autocorrelation between model and data. 

Our results demonstrate the possibility to fit long range autocorrelation with a Gaussian markov random field on an irregular grid. This confirms and extend, with a different methology, the results from Rue2002 on a regular field.

- theoretical paper on the link between kernel and autocorrelation see Rue2002, but difficult to read.

Next paper: can the same be done on a simulation model (almost stepping stone).
see Keeling2004 that demonstrate the possibility to evaluate spatial parameters from a single snapshot. 


### the fitting of Paucarpata
To be done soon, but on what model? Still a problem of streets mapping.

### the cofactors
- Simple general model for cofactors in 0/1: a normal effect for each cofactor when present 
- Ideally we need to be able to compare the quality of the prediction brought by cofactors and by the spatial component. Given that the spatial component can always be calculated, that is not the case of the cofactors.

### Interest of NA:
   1   NA
0  ?   1

The value of ? as a mean of the neighbourghood will be push toward 1 if we include the NA as it has a high probability to be 1.

This problem is very much in the grant: spatial pattern detection incertainty due to NA. And this way to deal with it can probably be adapted easily to the tesselation.

### the inspectors
The undidentifiability of inspectors (only the relative quality can be assessed) justify the use of a strong prior, it allows to fix the overall quality of inspectors, taking into account the variations between inspectors not to claim as streets differences the influence of inspectors.

Clearly the inspectors "remove information" from the data. A way to take them into account without making the effect of streets very weak could be to use the posterior without inspectors as a prior for the simulation with inspectors. The mean should not move in bad, NB, even with "loose prior", the effect of streets is present but the mean of beta is almost exactly the priors mean.

### the moving window
Interest: show that the influence of streets is not detectable the same when you are at the front of the wave.
Problem: How to control for the quantity of information put into the model: more infected houses more power so better detectability. Would controling for the number of infected houses enough? Isn't the geometry also important (influence of streets width for example). 
In a simulation framework I would generate the data in the different locations for the various parameters values found.

Here I would do on the whole map and then using the posterior for f and T as a prior I would look at the departure from this posterior when launched on subsets of the map.

### fitting the parasites the same way than the bugs
Interest: We can that way check that bugs and parasite have the same pattern, 
Problem: We have much less data, (80 houses max on one map, 40 houses on an other, maybe 2 or three other small maps). 
- Possibility to use all the maps successively to sharpen/place the prior progressively? 
- Again how to control for the quantity of information, in order to say that it's the same or not than the bugs? Can the standard deviation or other shape description of the posterior allow this?
- Probably the simplest would be to use the bugs prior on this map and see how much the parasite deviate from this prior. For example, we can do the succession of priors estimation for the bugs on this maps->posterior on the association for the bugs. Then do the same for the parasites but with as a prior the posterior for the bugs. If we deviate from this, especially on the side of less influence of the streets it means that they cross more easily the streets than the guinea pigs. If not it's an argument to say that the diffusion is mainly done by insects.

see for small data: la joya, the published article "Periurban Trypanosoma cruzi infectected Triatoma infestans, Arequipa, Peru.

### the importance of the size of streets
Interest: The same way, using the posterior on all streets, we can look at the impact of big streets versus small streets. The interest would especially be to check that when streets are narrow they are as much a barrier than when streets are bigger.
Problem: the size of the streets has not been mapped, and if mapped we would need to add a parameter to the model, for example we can have a kernel like T*S*exp(-fT) with S being 1 for small streets and x for big streets. The problem is also to say what is big. 
An other approach is to study two neighborhoods with the same rate of infection but with different streets structure. Like the two big patches in melgar I think. We can use the posterior as the prior and if the estimate in each one deviate on different sides we got our difference.

### accelaration of convergence (looks like not needed for now)
the other thing is that to accelerate convergence, we could make successive use of the meere gibbs sampler and then add T and f 
Generally, this would be the progressive introduction of the complexity in the model. 
or initially don't introduce "the noise" using samples2 or perfect inspectors/noNA then realise the influence of the noise and wait for stabilization
this also can be put in the grant on spatial uncertainty: in bayesian methods, introducing more noise lead to slower convergence, 
so we want a general approach that introduce each increment in the noise when the chain is stable (geweke)

It would be necessary to test again the results of the cosampling of f and T, even if it may open wider intervals arround f and T.

### relationship between T in spatial model and Delta in autocorrelation
The exponential kernel makes the Delta to be quite significant as:
exp(-a*(Delta+d))=exp(-a*Delta-a*d)=exp(-a*Delta)*exp(-a*d)
So in fact, T and Delta are striclty equivalent in the exponential model, T is only easier to identify, without applying the threshold to the out of distance.
for example, f=0.09527246 and T=0.05692508 correspond to:  <=> exp(-f*Delta)=T<=> Delta = - log(T)/f = 30 meters, kind of what we see in the autocorrelation curves, only smaller. The autocorrelation could have a magnification role. The only problem is the difficulties to reproduce this curves in the generated data, in the generated data, the two curves have difficulties to do the same thing and the order of magnitude would more from 40 to 150? meters.

## the kernel values found
f compared with known biology of the bug and map structure.
The short shape of the kernel, combined with a mean minimal distance favor a stepping stone model: the direct impact of a house is only on its direct neighbourgs.

## impropriety of the posteriors f and T
F and T are largely positively correlated. Moreover, when F is big enough to make the exponential being almost null before touching the first house we run into a problem of identifiability. 
Without T this would be easily cast out, but with T, the possibility that it gets huge values enable a "cross-association" accross the street
  |
o - o
  /     <- cross association accross the street here vertical in the middle
o - o
  |

Then T can perfectly compensate for any change in f, this is bad.

We then need to make f and or T proper to avoid to run into undidentifiability too easily. 

A reasonable way to set up the barrier could be to limit the log(T)/f rate but it may be a little complex to explain, it is probably the best way: we set a limit to how profitable the streets can be for the bugs. 

An other way is to use the data to give reasonable maximal value for f, for example an association of less than 5% for the mean min distance.

Probably the simplest and the fastest here is to use a biologically confortable prior for f: centered arround 

See definition_f_prior.r and associated pdf for the definition of the prior of f.

## the "confusion" between f and T
The fondamental problem is the possible non-identifiability of f and T.
The streets act as barriers through two complementary effects. First, the discontinuity they introduce can be a strong barrier in a nearly stepping stone process.
Second, they can represent a barrier by themselves, inducing a higher mortality of the bugs for example. The discontinuity effect is stronger when the association kernel present a sharp slope. To the contrary, the barrier effect is stronger when the association kernel allows for long distance direct association. Trying to identify at the same time the general slope of the association kernel and the barrier effect can prove to be hard. It is even harder on a very regular map when blocks are small squares, leaving few chance to test long range association within blocks and streets always are the same size leaving no chance to assess short distance effect of streets.

A way to get rid of the discontinuity effect on a regular map is to remove it... by putting the houses at the same distance whether they are separated by streets or not. On this the identification of f and T should be very efficient. So we can do that and then say "f and T can be less discernable when the streets are more and more a discontinuity, in both ways, to assess the impact of streets in the diffusion process one should look both at T and at the shape of the association kernel given the discontinuity introduced by streets. 

To explain and assess the effect of street as a whole, too calculus should be done. First the mean of the min distance of houses across streets, the same way than the mean of the dist of houses in same block will allow to compare the order of magnitude and help to explain the concept of discontinuity. Second the total sum of the effect of influence inside block and accross street should be calculated, giving overall the importance of fandT in the context of the map to separate allong the streets. The mean should also be compared as the number of pairs involved is very different in each group.  

## check of identifiability
use of a fake map with square blocks, 10x 10 blocks, inter houses space: 10m (the mean of minimal space on mariano melgar is 11.7: apply_by_row_not_null.spam(Dmat,min))
and we use nb_per_blocks<-aggregate(data$block_num,by=list(data$block_num),length) to get the number of houses per blocks, the mean being 10.6 in MM we will use 4 houses per side.

NB: using a regular map may make things even more difficult to identify, not having the full spectrum of the distances.

## interest of the model/sampler
It allows to look at house by house link and not at the general pattern, looking the same way at a 10 houses group and at at 1000 houses group on the same map. 
NB: under a very non informative map, or bad values of f (specially large values), T can really be doing whatever, so if doesn't find something, can fix f/have a stronger prior of f and see what happen.

## presentation of the results:
- Should use contour plot on f/T but also marginal distributions. It is also possible to generate the posterior distribution of the % of spatial component explained by same block houses, for a given simulation and for a lot of simulation he distribution of the mean. This would allow to discribe as a whole the effect of streets and possibly to show that the variation of the overall effect of street is less than the individual variations of f and T.
- a map of the mean spatial component but also the sd of the spatial component (accross iterations) should be produced.
- strong point of the model, allow to get an idea of the dispersion on cross sectional data
- not a simulation model but a statistical model: fast implementation
- the geometry used to describe the data matters, so subsequent simulation of the process generating the data should either respect the spatial homogeneity (Holland2007) or the spatial heterogeneity of the space.
Here we present a way to test for spatial connectivity of cross-sectional data using an auto-regressive gaussian random field. This analysis allow us to identify the city block as unit of aggregation and thus probably of dispersion. We are able to evaluate how much and how streets structurate the aggregation of the vectors.
- first known attempt to describe an highly structured environment with a Gaussian /Gaussian Markov random field. Despite growing utilisation.
- example of cross sectional method to infer epidemiological patterns, but without structuration:Rorres2010

## about detection/control of the bugs
Streets having a major impact on the aggregation means that:
- city blocks should be considered as units of detection and control
Short kernel suggest more a stepping stone process than a long range process, at least for inside block propagation.
